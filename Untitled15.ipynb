{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMd0yK/JaSGS8otRp0QLG3l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daschirag/Smart-AI-academy/blob/main/Untitled15.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ED68CbwZKjG_"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "categories = [\n",
        "    \"Consistent Performer\", \"Late Bloomer\", \"Strategist\",\n",
        "    \"Conceptual Master\", \"Speedster\", \"Under-Pressure Performer\",\n",
        "    \"Resourceful Learner\", \"Emotionally Stable\", \"Stress Sensitive\"\n",
        "]\n",
        "\n",
        "answer_encoding = {'a': 0, 'b': 1, 'c': 2, 'd': 3}\n",
        "\n",
        "questions_mapping = [\n",
        "    {\n",
        "        'question': \"1. How do you manage your daily study schedule?\",\n",
        "        'options': {\n",
        "            'a': (\"I follow a strict daily schedule with dedicated time slots for each subject.\", {\"Consistent Performer\": 2, \"Strategist\": 1}),\n",
        "            'b': (\"I try to study when I can, but often end up cramming at the last minute.\", {\"Late Bloomer\": 2}),\n",
        "            'c': (\"I have a loose plan and frequently change topics based on available resources.\", {\"Resourceful Learner\": 2, \"Conceptual Master\": 1}),\n",
        "            'd': (\"I rely on coaching classes and rarely plan on my own.\", {\"Under-Pressure Performer\": 2, \"Strategist\": 1})\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'question': \"2. When faced with difficult JEE problems, how do you proceed?\",\n",
        "        'options': {\n",
        "            'a': (\"I analyze the problem step by step and explore multiple approaches.\", {\"Conceptual Master\": 2, \"Consistent Performer\": 1}),\n",
        "            'b': (\"I quickly look for shortcuts, even if I'm not completely sure.\", {\"Speedster\": 2}),\n",
        "            'c': (\"I ask for help from friends or online forums.\", {\"Resourceful Learner\": 2}),\n",
        "            'd': (\"I get overwhelmed and tend to skip the problem.\", {\"Under-Pressure Performer\": 2, \"Stress Sensitive\": 1})\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'question': \"3. What is your revision strategy as the exam approaches?\",\n",
        "        'options': {\n",
        "            'a': (\"I revise topics consistently over time using summary notes and flashcards.\", {\"Consistent Performer\": 2, \"Strategist\": 1}),\n",
        "            'b': (\"I try to revise everything in the last month, cramming as much as possible.\", {\"Late Bloomer\": 2}),\n",
        "            'c': (\"I rely on group study sessions and online test series to revise.\", {\"Resourceful Learner\": 2, \"Conceptual Master\": 1}),\n",
        "            'd': (\"I depend solely on coaching classes for revision.\", {\"Under-Pressure Performer\": 2})\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'question': \"4. How do you handle exam stress and anxiety?\",\n",
        "        'options': {\n",
        "            'a': (\"I have developed routines like meditation and exercise to stay calm.\", {\"Emotionally Stable\": 2, \"Consistent Performer\": 1}),\n",
        "            'b': (\"I feel nervous but manage it with deep breathing.\", {\"Under-Pressure Performer\": 2, \"Stress Sensitive\": 1}),\n",
        "            'c': (\"I feel very anxious and sometimes skip practice tests.\", {\"Stress Sensitive\": 2}),\n",
        "            'd': (\"I actually perform better under pressure.\", {\"Under-Pressure Performer\": 2, \"Strategist\": 1})\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'question': \"5. During practice tests, how effectively do you manage your time?\",\n",
        "        'options': {\n",
        "            'a': (\"I plan and allocate time well for each section.\", {\"Consistent Performer\": 2, \"Strategist\": 1}),\n",
        "            'b': (\"I sometimes rush, leading to careless mistakes.\", {\"Speedster\": 2}),\n",
        "            'c': (\"I tend to overthink certain questions and run out of time.\", {\"Late Bloomer\": 2, \"Under-Pressure Performer\": 1}),\n",
        "            'd': (\"I consistently struggle with time management and feel very pressured.\", {\"Under-Pressure Performer\": 2, \"Stress Sensitive\": 1})\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'question': \"6. How confident are you in your grasp of core JEE concepts?\",\n",
        "        'options': {\n",
        "            'a': (\"I am confident in my understanding and can apply concepts well.\", {\"Conceptual Master\": 2, \"Consistent Performer\": 1}),\n",
        "            'b': (\"I understand most concepts but sometimes get stuck in application.\", {\"Resourceful Learner\": 2}),\n",
        "            'c': (\"I tend to rely more on memorization rather than true understanding.\", {\"Under-Pressure Performer\": 2}),\n",
        "            'd': (\"I frequently struggle with fundamentals.\", {\"Late Bloomer\": 2})\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'question': \"7. How do you respond to feedback after practice tests?\",\n",
        "        'options': {\n",
        "            'a': (\"I analyze my mistakes and adjust my study plan accordingly.\", {\"Consistent Performer\": 2, \"Strategist\": 1}),\n",
        "            'b': (\"I feel disappointed but try to learn from it.\", {\"Resourceful Learner\": 2}),\n",
        "            'c': (\"I get demotivated and lose focus.\", {\"Under-Pressure Performer\": 2, \"Stress Sensitive\": 1}),\n",
        "            'd': (\"I ignore feedback and stick to my routine.\", {\"Late Bloomer\": 2})\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'question': \"8. What best describes your overall exam preparation approach?\",\n",
        "        'options': {\n",
        "            'a': (\"I maintain a balanced study routine covering all subjects.\", {\"Consistent Performer\": 2, \"Strategist\": 1}),\n",
        "            'b': (\"I focus on subjects I find easier and neglect tougher ones.\", {\"Late Bloomer\": 2}),\n",
        "            'c': (\"I experiment with various strategies but lack a fixed plan.\", {\"Resourceful Learner\": 2, \"Conceptual Master\": 1}),\n",
        "            'd': (\"I depend heavily on coaching classes and external guidance.\", {\"Under-Pressure Performer\": 2})\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'question': \"9. How would you rate your overall physical and mental stamina during exam preparation?\",\n",
        "        'options': {\n",
        "            'a': (\"I feel energetic and balanced.\", {\"Emotionally Stable\": 2, \"Consistent Performer\": 1}),\n",
        "            'b': (\"I sometimes feel exhausted, but I push through.\", {\"Under-Pressure Performer\": 2, \"Stress Sensitive\": 1}),\n",
        "            'c': (\"I often feel drained, which affects my concentration.\", {\"Stress Sensitive\": 2}),\n",
        "            'd': (\"I consistently struggle with fatigue and stress.\", {\"Late Bloomer\": 2, \"Stress Sensitive\": 1})\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'question': \"10. How do you react to setbacks or poor performance in practice tests?\",\n",
        "        'options': {\n",
        "            'a': (\"I view setbacks as opportunities to learn and improve.\", {\"Consistent Performer\": 2, \"Emotionally Stable\": 1}),\n",
        "            'b': (\"I feel disappointed but try to recover quickly.\", {\"Late Bloomer\": 2, \"Stress Sensitive\": 1}),\n",
        "            'c': (\"I remain calm, analyze my mistakes, and adjust my approach.\", {\"Strategist\": 2, \"Emotionally Stable\": 1}),\n",
        "            'd': (\"I get very frustrated, which negatively impacts my preparation.\", {\"Under-Pressure Performer\": 2, \"Stress Sensitive\": 1})\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "num_questions = 10\n",
        "\n",
        "def get_user_answers():\n",
        "    answers = []\n",
        "    print(\"Welcome to the JEE Student Classification and SOCA Analysis System!\\n\")\n",
        "    for i, q in enumerate(questions_mapping):\n",
        "        print(q['question'])\n",
        "        for key, (option_text, _) in q['options'].items():\n",
        "            print(f\"  {key.upper()}. {option_text}\")\n",
        "        while True:\n",
        "            user_ans = input(\"Enter your choice (a/b/c/d): \").strip().lower()\n",
        "            if user_ans in q['options']:\n",
        "                answers.append(user_ans)\n",
        "                break\n",
        "            else:\n",
        "                print(\"Invalid input. Please enter a, b, c, or d.\")\n",
        "        print()\n",
        "    return answers\n",
        "\n",
        "def compute_category_scores(answers):\n",
        "    scores = {category: 0 for category in categories}\n",
        "    for i, answer in enumerate(answers):\n",
        "        mapping = questions_mapping[i]\n",
        "        if answer in mapping:\n",
        "            for cat, score in mapping[answer].items():\n",
        "                scores[cat] += score\n",
        "    return scores\n",
        "\n",
        "def get_label_from_answers(answers):\n",
        "    scores = compute_category_scores(answers)\n",
        "    max_score = max(scores.values())\n",
        "    best_categories = sorted([cat for cat, score in scores.items() if score == max_score])\n",
        "    return best_categories[0], scores\n",
        "\n",
        "def generate_synthetic_dataset(n_samples=1000, random_state=42):\n",
        "    random.seed(random_state)\n",
        "    np.random.seed(random_state)\n",
        "    X, y = [], []\n",
        "    possible_answers = ['a', 'b', 'c', 'd']\n",
        "    for _ in range(n_samples):\n",
        "        sample_answers = [random.choice(possible_answers) for _ in range(num_questions)]\n",
        "        encoded = [answer_encoding[ans] for ans in sample_answers]\n",
        "        label, _ = get_label_from_answers(sample_answers)\n",
        "        X.append(encoded)\n",
        "        y.append(label)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X, y = generate_synthetic_dataset(n_samples=1000)\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "test_accuracy = clf.score(X_test, y_test)\n",
        "print(f\"Model Test Accuracy: {test_accuracy:.2f}\\n\")\n",
        "\n",
        "def predict_category_for_answers(answers):\n",
        "    encoded = [answer_encoding[ans] for ans in answers]\n",
        "    prediction = clf.predict([encoded])[0]\n",
        "    predicted_category = label_encoder.inverse_transform([prediction])[0]\n",
        "    return predicted_category\n",
        "\n",
        "def get_sentiment_score(free_text):\n",
        "    return sia.polarity_scores(free_text)[\"compound\"]\n",
        "\n",
        "def display_soca_report(final_category, scores, sentiment):\n",
        "    if final_category == \"Consistent Performer\":\n",
        "        report = f\"----- Detailed SOCA Analysis for Consistent Performer -----\\n\\nStrengths:\\n- High discipline and regular study habits.\\n- Consistent progress and thorough revision techniques.\\n- Effective time management and reliable performance.\\n\\nOpportunities:\\n- Enhance speed and accuracy through targeted drills.\\n- Incorporate advanced problem-solving techniques.\\n- Leverage mock test analysis to identify weak areas.\\n\\nChallenges:\\n- Risk of plateauing due to comfort in routine.\\n- May avoid extremely challenging problems.\\n- Limited flexibility in adapting to unexpected exam patterns.\\n\\nAction Plan & Road Map:\\n1. Initial Phase (0-1 Month):\\n   - Establish a rigorous daily schedule and take baseline mock tests.\\n   - Identify weak areas and address them through focused practice.\\n2. Improvement Phase (1-3 Months):\\n   - Increase mock test frequency and maintain an error log.\\n   - Enroll in advanced coaching or online modules.\\n3. Peak Performance Phase (3-6 Months):\\n   - Solve high-difficulty questions and simulate exam conditions.\\n   - Refine time management and test-taking strategies.\\n4. Final Preparation (Last 1 Month):\\n   - Consolidate learning and focus on revision.\\n   - Maintain physical and mental well-being.\\n\\nRecommended Courses:\\n- Coaching: Advanced modules from Allen or FIITJEE.\\n- Online: Unacademy's Masterclass, Vedantu's Booster Sessions.\\n- Materials: NCERT fundamentals and specialized texts.\\n\\nMatch Percentage: {max(scores.values())/20*100:.1f}%\\nFree-Text Sentiment Score: {sentiment:.2f}\\n\"\n",
        "        print(report)\n",
        "    elif final_category == \"Late Bloomer\":\n",
        "        report = f\"----- Detailed SOCA Analysis for Late Bloomer -----\\n\\nStrengths:\\n- High potential for rapid improvement.\\n- Capable of absorbing large volumes of information quickly.\\n- Performs well under last-minute revision pressure.\\n\\nOpportunities:\\n- Develop a structured daily study plan.\\n- Improve consistency with regular, planned revision sessions.\\n- Use mock tests to simulate exam pressure and build momentum.\\n\\nChallenges:\\n- Procrastination and delayed progress.\\n- Difficulty building momentum early in preparation.\\n- High stress during last-minute cramming.\\n\\nAction Plan & Road Map:\\n1. Initial Phase (0-1 Month):\\n   - Set a fixed daily schedule and take baseline mock tests.\\n   - Begin revising fundamental concepts and maintain a progress journal.\\n2. Improvement Phase (1-3 Months):\\n   - Focus on weak areas with targeted practice.\\n   - Take weekly topic-wise tests and use the Pomodoro technique to enhance focus.\\n3. Peak Performance Phase (3-6 Months):\\n   - Engage in full-length mock tests under timed conditions.\\n   - Analyze errors and adjust study plans accordingly.\\n4. Final Preparation (Last 1 Month):\\n   - Consolidate learning with intensive revision.\\n   - Solve previous years’ JEE papers and prioritize stress management.\\n\\nRecommended Courses:\\n- Coaching: FIITJEE’s Intensive Program, Aakash’s Rapid Revision.\\n- Online: Vedantu’s Booster Sessions, Unacademy's Last Mile Strategy.\\n- Materials: Focus on NCERT for Chemistry and advanced texts.\\n\\nMatch Percentage: {max(scores.values())/20*100:.1f}%\\nFree-Text Sentiment Score: {sentiment:.2f}\\n\"\n",
        "        print(report)\n",
        "    else:\n",
        "        print(f\"SOCA analysis for {final_category} is not available in detailed form yet. Please refer to general recommendations.\")\n",
        "\n",
        "def main():\n",
        "    user_answers = get_user_answers()\n",
        "    free_text = input(\"Describe your overall study experience in 2-3 sentences: \").strip()\n",
        "    final_category, raw_scores = get_label_from_answers(user_answers)\n",
        "    predicted_category = predict_category_for_answers(user_answers)\n",
        "    sentiment = get_sentiment_score(free_text)\n",
        "    print(\"\\n----- Overall Classification Results -----\")\n",
        "    print(\"Predicted JEE Student Category:\", predicted_category)\n",
        "    print(\"Raw Category Scores:\", raw_scores)\n",
        "    print(f\"Free-Text Sentiment Score: {sentiment:.2f}\\n\")\n",
        "    display_soca_report(predicted_category, raw_scores, sentiment)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from datasets import Dataset\n",
        "\n",
        "# -------------------------------\n",
        "# Step 1: Create a Fine-Tuning Dataset\n",
        "# -------------------------------\n",
        "# Each example consists of a prompt (indicating the SOC category)\n",
        "# and a response (a detailed study roadmap).\n",
        "data = [\n",
        "    {\n",
        "        \"prompt\": \"SOC: A\\nDetailed Study Roadmap:\",\n",
        "        \"response\": (\n",
        "            \"High resource students: Study 6-8 hours daily. \"\n",
        "            \"Allocate 2 hours for Physics (advanced problem solving), \"\n",
        "            \"1.5 hours for Chemistry (balancing inorganic, organic, and physical), \"\n",
        "            \"and 2 hours for Mathematics (challenging problems practice). \"\n",
        "            \"Include weekly full-length mock tests and monthly revision sessions with advanced problems.\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"SOC: B\\nDetailed Study Roadmap:\",\n",
        "        \"response\": (\n",
        "            \"Moderate resource students: Study 5-6 hours daily. \"\n",
        "            \"Spend 1.5 hours on Physics (focusing on fundamentals and numerical practice), \"\n",
        "            \"1 hour on Chemistry (using NCERT for core concepts), \"\n",
        "            \"and 1.5 hours on Mathematics (consistent practice). \"\n",
        "            \"Incorporate weekly online mock tests, join peer study groups, and focus on optimizing resources and time management.\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"SOC: C\\nDetailed Study Roadmap:\",\n",
        "        \"response\": (\n",
        "            \"Limited resource students: Study 4-5 hours daily. \"\n",
        "            \"Dedicate 1 hour to Physics (basic concepts and essential problem solving), \"\n",
        "            \"1 hour to Chemistry (key concepts and reactions), \"\n",
        "            \"and 1 hour to Mathematics (emphasizing problem-solving techniques). \"\n",
        "            \"Leverage community resources, free online coaching, and group studies; explore scholarships and maintain a healthy routine.\"\n",
        "        )\n",
        "    }\n",
        "]\n",
        "\n",
        "# Combine prompt and response into one text per sample.\n",
        "def combine_prompt_response(example):\n",
        "    return example[\"prompt\"] + \"\\n\" + example[\"response\"]\n",
        "\n",
        "texts = [combine_prompt_response(item) for item in data]\n",
        "\n",
        "# Create a Hugging Face dataset from the list of texts.\n",
        "dataset = Dataset.from_dict({\"text\": texts})\n",
        "\n",
        "# -------------------------------\n",
        "# Step 2: Load the Pretrained Model and Tokenizer\n",
        "# -------------------------------\n",
        "model_name = \"gpt2\"  # Replace with your model (e.g., \"Mistral-7B-Instruct\") when ready.\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# GPT-2 does not have an explicit padding token, so we set it to eos_token.\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# -------------------------------\n",
        "# Step 3: Tokenize the Dataset\n",
        "# -------------------------------\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# -------------------------------\n",
        "# Step 4: Data Collator for Language Modeling\n",
        "# -------------------------------\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# -------------------------------\n",
        "# Step 5: Define Training Arguments\n",
        "# -------------------------------\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=5,               # Increase epochs for larger datasets\n",
        "    per_device_train_batch_size=2,\n",
        "    save_steps=10,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=5,\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Step 6: Initialize the Trainer\n",
        "# -------------------------------\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Step 7: Fine-Tune the Model\n",
        "# -------------------------------\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model for later use.\n",
        "model.save_pretrained(\"./fine_tuned_model\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
        "\n",
        "# -------------------------------\n",
        "# Step 8: Generate a Detailed Study Roadmap Given a SOC Category\n",
        "# -------------------------------\n",
        "def generate_roadmap(soc_category):\n",
        "    input_text = f\"SOC: {soc_category}\\nDetailed Study Roadmap:\"\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate text using the fine-tuned model\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_length=300,\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# -------------------------------\n",
        "# Main: Get User Input and Display the Generated Roadmap\n",
        "# -------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    soc_input = input(\"Enter Socio-economic category (A, B, C): \").strip().upper()\n",
        "    roadmap = generate_roadmap(soc_input)\n",
        "    print(\"\\nGenerated Detailed Study Roadmap:\\n\")\n",
        "    print(roadmap)\n",
        "#this is an example with using nlp we can expand the more and provide links to it"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "5AWX1_1QNMqt",
        "outputId": "f32378fb-fd96-477b-d94e-d665f4c4b72f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a8b0658a4f03>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataCollatorForLanguageModeling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# -------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2485\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2486\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m \u001b[0;31m# Enable CUDA Sanitizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_meta_registrations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prims_common\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSymBool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSymFloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m from torch._decomp import (\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0m_add_op_to_registry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0m_convert_out_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_decomp/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;31m# populate the table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decomp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompositions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_refs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_refs/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2549\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mregister_decomposition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2550\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mout_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2551\u001b[0m @elementwise_type_promotion_wrapper(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_decomp/__init__.py\u001b[0m in \u001b[0;36mdecomposition_decorator\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;31m# To handle allowing multiple aten_ops at once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mpytree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_map_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maten_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0morig_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_pytree.py\u001b[0m in \u001b[0;36mtree_map_\u001b[0;34m(func, tree, is_leaf, *rests)\u001b[0m\n\u001b[1;32m    993\u001b[0m         \u001b[0;32min\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0mof\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mat\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mat\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mnodes\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mrests\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m     \"\"\"\n\u001b[0;32m--> 995\u001b[0;31m     \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreespec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    996\u001b[0m     \u001b[0mflat_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtreespec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrests\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mflat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# consume and exhaust the iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_pytree.py\u001b[0m in \u001b[0;36mtree_flatten\u001b[0;34m(tree, is_leaf)\u001b[0m\n\u001b[1;32m    872\u001b[0m     \"\"\"\n\u001b[1;32m    873\u001b[0m     \u001b[0mleaves\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m     \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tree_flatten_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_pytree.py\u001b[0m in \u001b[0;36m_tree_flatten_helper\u001b[0;34m(tree, leaves, is_leaf)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m def _tree_flatten_helper(\n\u001b[0m\u001b[1;32m    846\u001b[0m     \u001b[0mtree\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPyTree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m     \u001b[0mleaves\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import (\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    get_scheduler,\n",
        ")\n",
        "from datasets import Dataset\n",
        "import wandb\n",
        "from peft import get_peft_model, LoraConfig  # PEFT: Parameter Efficient Fine-Tuning using LoRA\n",
        "\n",
        "# Initialize Weights & Biases for experiment tracking.\n",
        "wandb.init(\n",
        "    project=\"jee_mentor_finetuning\",\n",
        "    entity=\"your_entity_name\",  # Replace with your W&B username or organization\n",
        "    config={\n",
        "        \"epochs\": 5,\n",
        "        \"batch_size\": 2,\n",
        "        \"learning_rate\": 5e-5,\n",
        "        \"model\": \"gpt2\",\n",
        "        \"lora_r\": 8,\n",
        "        \"lora_alpha\": 16,\n",
        "    }\n",
        ")\n",
        "\n",
        "# -------------------------------------\n",
        "# Step 1: Create a Fine-Tuning Dataset\n",
        "# -------------------------------------\n",
        "# Each example contains a prompt and a detailed study roadmap response.\n",
        "data = [\n",
        "    {\n",
        "        \"prompt\": \"SOC: A\\nDetailed Study Roadmap:\",\n",
        "        \"response\": (\n",
        "            \"High resource students: Study 6-8 hours daily. \"\n",
        "            \"Allocate 2 hours for Physics (advanced problem solving), \"\n",
        "            \"1.5 hours for Chemistry (balancing inorganic, organic, and physical), \"\n",
        "            \"and 2 hours for Mathematics (challenging problem practice). \"\n",
        "            \"Include weekly full-length mock tests and monthly advanced revision sessions.\"\n",
        "        ),\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"SOC: B\\nDetailed Study Roadmap:\",\n",
        "        \"response\": (\n",
        "            \"Moderate resource students: Study 5-6 hours daily. \"\n",
        "            \"Spend 1.5 hours on Physics (focusing on fundamentals and numerical practice), \"\n",
        "            \"1 hour on Chemistry (using NCERT for core concepts), \"\n",
        "            \"and 1.5 hours on Mathematics (consistent practice). \"\n",
        "            \"Incorporate weekly online mock tests, join peer study groups, and optimize time management.\"\n",
        "        ),\n",
        "    },\n",
        "    {\n",
        "        \"prompt\": \"SOC: C\\nDetailed Study Roadmap:\",\n",
        "        \"response\": (\n",
        "            \"Limited resource students: Study 4-5 hours daily. \"\n",
        "            \"Dedicate 1 hour to Physics (covering basic principles and essential problem solving), \"\n",
        "            \"1 hour to Chemistry (focusing on key concepts and reactions), \"\n",
        "            \"and 1 hour to Mathematics (emphasizing problem-solving techniques). \"\n",
        "            \"Utilize community resources, free online coaching, and group studies; explore scholarships and maintain a balanced routine.\"\n",
        "        ),\n",
        "    },\n",
        "]\n",
        "\n",
        "# Combine prompt and response for each sample.\n",
        "def combine_prompt_response(example):\n",
        "    return example[\"prompt\"] + \"\\n\" + example[\"response\"]\n",
        "\n",
        "texts = [combine_prompt_response(item) for item in data]\n",
        "dataset = Dataset.from_dict({\"text\": texts})\n",
        "\n",
        "# -------------------------------------\n",
        "# Step 2: Load the Pretrained Model and Tokenizer\n",
        "# -------------------------------------\n",
        "model_name = \"gpt2\"  # For demonstration; replace with your target model such as \"Mistral-7B-Instruct\" in production.\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Set tokenizer's pad token to eos_token (GPT2 does not have a dedicated padding token).\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# -------------------------------------\n",
        "# Step 3: Apply PEFT with LoRA Configuration\n",
        "# -------------------------------------\n",
        "lora_config = LoraConfig(\n",
        "    r=wandb.config.lora_r,\n",
        "    lora_alpha=wandb.config.lora_alpha,\n",
        "    target_modules=[\"c_attn\"],  # This can be adjusted depending on model architecture.\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(\"LoRA modules injected:\", model.print_trainable_parameters())\n",
        "\n",
        "# -------------------------------------\n",
        "# Step 4: Tokenize the Dataset\n",
        "# -------------------------------------\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# -------------------------------------\n",
        "# Step 5: Create a Data Collator for Language Modeling\n",
        "# -------------------------------------\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# -------------------------------------\n",
        "# Step 6: Define Training Arguments with Dynamic LR Scheduler and W&B Logging\n",
        "# -------------------------------------\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=wandb.config.epochs,\n",
        "    per_device_train_batch_size=wandb.config.batch_size,\n",
        "    learning_rate=wandb.config.learning_rate,\n",
        "    logging_steps=5,\n",
        "    save_steps=10,\n",
        "    save_total_limit=2,\n",
        "    evaluation_strategy=\"no\",\n",
        "    report_to=[\"wandb\"],\n",
        "    fp16=torch.cuda.is_available(),  # Use mixed precision if available.\n",
        ")\n",
        "\n",
        "# -------------------------------------\n",
        "# Step 7: Initialize the Trainer\n",
        "# -------------------------------------\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# -------------------------------------\n",
        "# Step 8: Fine-Tune the Model\n",
        "# -------------------------------------\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model for later use.\n",
        "model.save_pretrained(\"./fine_tuned_model\")\n",
        "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
        "\n",
        "# -------------------------------------\n",
        "# Step 9: Generate a Detailed Study Roadmap Using the Fine-Tuned Model\n",
        "# -------------------------------------\n",
        "def generate_roadmap(soc_category):\n",
        "    input_text = f\"SOC: {soc_category}\\nDetailed Study Roadmap:\"\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate text using the fine-tuned model with advanced generation parameters.\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_length=300,\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True,\n",
        "        do_sample=True,           # Sampling for diverse outputs.\n",
        "        top_p=0.95,               # Nucleus sampling.\n",
        "        temperature=0.8,          # Adjust temperature for creativity.\n",
        "    )\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# -------------------------------------\n",
        "# Main: Get User Input and Display the Generated Roadmap\n",
        "# -------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    soc_input = input(\"Enter Socio-economic category (A, B, C): \").strip().upper()\n",
        "    roadmap = generate_roadmap(soc_input)\n",
        "    print(\"\\nGenerated Detailed Study Roadmap:\\n\")\n",
        "    print(roadmap)\n"
      ],
      "metadata": {
        "id": "eq9qQASYNlJl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}